import asyncio
from fastapi import FastAPI, UploadFile, Form, File, Header, WebSocket, WebSocketDisconnect
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
from datetime import datetime, timedelta
import csv
import os
import json
from ultralytics import YOLO
import threading
# uvicorn main:app --reload

message_queue = asyncio.Queue()

main_event_loop = None

app = FastAPI()

# æ·»åŠ  CORS ä¸­é—´ä»¶ï¼Œå…è®¸æ‰€æœ‰æ¥æºçš„è¯·æ±‚
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # å…è®¸æ‰€æœ‰æ¥æº
    allow_credentials=True,
    allow_methods=["*"],  # å…è®¸æ‰€æœ‰ HTTP æ–¹æ³•
    allow_headers=["*"],  # å…è®¸æ‰€æœ‰ HTTP è¯·æ±‚å¤´
)

# å®šä¹‰è¯·æ±‚ä½“ç»“æ„
class DatasetQuery(BaseModel):
    page: int
    page_size: int
    task: Optional[str] = None  # æ·»åŠ ä»»åŠ¡ç±»å‹ç­›é€‰å‚æ•°

# æ„é€ ä¼ªæ•°æ®
def generate_fake_dataset(index: int) -> dict:
    # æ ¹æ®ç´¢å¼•ç¡®å®šä»»åŠ¡ç±»å‹
    task_types = ["detect", "classify", "segment", "pose", "obb"]
    task_type = task_types[index % len(task_types)]
    
    # æ ¹æ®ä»»åŠ¡ç±»å‹ç”Ÿæˆä¸åŒçš„æ•°æ®é›†
    if task_type == "detect":
        return {
            "id": index,
            "creator_id": 1001,
            "name": f"SAR_Dataset_{index}",
            "task": "detect",
            "class_count": 5,
            "classes": [
                {"id": 0, "name": "ship"},
                {"id": 1, "name": "vehicle"},
                {"id": 2, "name": "building"},
                {"id": 3, "name": "airplane"},
                {"id": 4, "name": "bridge"},
            ],
            "description": "SARç›®æ ‡æ£€æµ‹æ•°æ®é›†",
            "path": f"D:/datasets/sar_detect_{index}",
            "image_count": 1000,
            "train_count": 800,
            "val_count": 100,
            "test_count": 100,
            "size": 102400000,
            "upload_time": (datetime(2025, 3, 25, 12, 0, 0) + timedelta(days=index)).isoformat() + "Z",
        }
    elif task_type == "classify":
        return {
            "id": index,
            "creator_id": 1001,
            "name": f"SAR_Classify_Dataset_{index}",
            "task": "classify",
            "class_count": 10,
            "classes": [
                {"id": 0, "name": "urban"},
                {"id": 1, "name": "rural"},
                {"id": 2, "name": "forest"},
                {"id": 3, "name": "water"},
                {"id": 4, "name": "desert"},
                {"id": 5, "name": "mountain"},
                {"id": 6, "name": "coastline"},
                {"id": 7, "name": "agriculture"},
                {"id": 8, "name": "industrial"},
                {"id": 9, "name": "residential"},
            ],
            "description": "SARå›¾åƒåˆ†ç±»æ•°æ®é›†",
            "path": f"D:/datasets/sar_classify_{index}",
            "image_count": 2000,
            "train_count": 1600,
            "val_count": 200,
            "test_count": 200,
            "size": 204800000,
            "upload_time": (datetime(2025, 3, 25, 12, 0, 0) + timedelta(days=index)).isoformat() + "Z",
        }
    elif task_type == "segment":
        return {
            "id": index,
            "creator_id": 1001,
            "name": f"SAR_Segment_Dataset_{index}",
            "task": "segment",
            "class_count": 8,
            "classes": [
                {"id": 0, "name": "road"},
                {"id": 1, "name": "building"},
                {"id": 2, "name": "water"},
                {"id": 3, "name": "vegetation"},
                {"id": 4, "name": "bare_soil"},
                {"id": 5, "name": "vehicle"},
                {"id": 6, "name": "ship"},
                {"id": 7, "name": "airplane"},
            ],
            "description": "SARå›¾åƒåˆ†å‰²æ•°æ®é›†",
            "path": f"D:/datasets/sar_segment_{index}",
            "image_count": 1500,
            "train_count": 1200,
            "val_count": 150,
            "test_count": 150,
            "size": 153600000,
            "upload_time": (datetime(2025, 3, 25, 12, 0, 0) + timedelta(days=index)).isoformat() + "Z",
        }
    elif task_type == "pose":
        return {
            "id": index,
            "creator_id": 1001,
            "name": f"SAR_Pose_Dataset_{index}",
            "task": "pose",
            "class_count": 3,
            "classes": [
                {"id": 0, "name": "person"},
                {"id": 1, "name": "vehicle"},
                {"id": 2, "name": "aircraft"},
            ],
            "description": "SARå§¿æ€ä¼°è®¡æ•°æ®é›†",
            "path": f"D:/datasets/sar_pose_{index}",
            "image_count": 800,
            "train_count": 640,
            "val_count": 80,
            "test_count": 80,
            "size": 81920000,
            "upload_time": (datetime(2025, 3, 25, 12, 0, 0) + timedelta(days=index)).isoformat() + "Z",
        }
    else:  # obb (Oriented Bounding Box)
        return {
            "id": index,
            "creator_id": 1001,
            "name": f"SAR_OBB_Dataset_{index}",
            "task": "obb",
            "class_count": 4,
            "classes": [
                {"id": 0, "name": "ship"},
                {"id": 1, "name": "airplane"},
                {"id": 2, "name": "bridge"},
                {"id": 3, "name": "vehicle"},
            ],
            "description": "SARæ—‹è½¬æ¡†æ£€æµ‹æ•°æ®é›†",
            "path": f"D:/datasets/sar_obb_{index}",
            "image_count": 1200,
            "train_count": 960,
            "val_count": 120,
            "test_count": 120,
            "size": 122880000,
            "upload_time": (datetime(2025, 3, 25, 12, 0, 0) + timedelta(days=index)).isoformat() + "Z",
        }

fake_datasets = [generate_fake_dataset(i) for i in range(1, 51)]  # 50 æ¡æ•°æ®

@app.post("/v1/dataset/get/")
async def get_datasets(query: DatasetQuery, authorization: Optional[str] = Header(None)):
    print("ğŸ“¥ æ”¶åˆ°æ•°æ®é›†åˆ—è¡¨è¯·æ±‚:")
    print(f"- é¡µç : {query.page}, æ¯é¡µæ•°é‡: {query.page_size}")
    print(f"- ä»»åŠ¡ç±»å‹: {query.task}")
    print(f"- æˆæƒå¤´: {authorization}")

    # æ ¹æ®ä»»åŠ¡ç±»å‹ç­›é€‰æ•°æ®é›†
    filtered_datasets = fake_datasets
    if query.task:
        filtered_datasets = [dataset for dataset in fake_datasets if dataset["task"] == query.task]

    start = (query.page - 1) * query.page_size
    end = start + query.page_size
    paged_data = filtered_datasets[start:end]

    return JSONResponse(content={
        "code": 200,
        "message": "è¯·æ±‚æˆåŠŸ",
        "total": len(filtered_datasets),
        "page": query.page,
        "page_size": query.page_size,
        "datasets": paged_data
    })





@app.post("/v1/dataset/upload/")
async def upload_dataset(
    name: str = Form(...),
    task: str = Form(...),
    file: UploadFile = File(...),
    description: str = Form(''),
    authorization: str = Header(None)
):
    print("ğŸ“ æ¥æ”¶åˆ°è¯·æ±‚:")
    print(f"- åç§°: {name}")
    print(f"- ä»»åŠ¡ç±»å‹: {task}")
    print(f"- æè¿°: {description}")
    print(f"- æ–‡ä»¶å: {file.filename}")
    print(f"- æˆæƒå¤´: {authorization}")

    # è¿™é‡Œåªæ˜¯ç¡®è®¤æ¥æ”¶åˆ°äº†è¯·æ±‚ï¼Œä¸åšå®é™…ä¿å­˜å¤„ç†
    return JSONResponse(content={
        "code": 200,
        "message": "æ•°æ®é›†ä¸Šä¼ æ¥å£å·²æ¥æ”¶è¯·æ±‚",
        "filename": file.filename
    })

class ModelQuery(BaseModel):
    filter: Optional[dict] = {}
    page: int = 1
    page_size: int = 10

def generate_fake_model(index: int) -> dict:
    return {
        "id": 100 + index,
        "name": f"SAR_Detect_Model_{index}",
        "task": "detect",
        "status": "trained",
        "creator_id": 500,
        "create_time": (datetime(2025, 3, 24, 10, 0, 0) + timedelta(days=index)).isoformat() + "Z",
        "description": "SARç›®æ ‡æ£€æµ‹æ¨¡å‹",
        "project_id": 1,
        "path": f"D:/models/sar_detect_v{index}.pth",
        "size": round(10.0 + index * 1000000, 2),  # æ¨¡æ‹Ÿä¸åŒå¤§å°
        "weight_type": "float32"
    }


fake_models = [generate_fake_model(i) for i in range(50)]

@app.post("/v1/model/get")
async def get_models(query: ModelQuery, authorization: Optional[str] = Header(None)):
    print("ğŸ“¥ æ”¶åˆ°æ¨¡å‹åˆ—è¡¨è¯·æ±‚:")
    print(f"- ç­›é€‰æ¡ä»¶: {query.filter}")
    print(f"- é¡µç : {query.page}, æ¯é¡µæ•°é‡: {query.page_size}")
    print(f"- æˆæƒå¤´: {authorization}")

    filtered = [
        m for m in fake_models
        if all(m.get(k) == v for k, v in query.filter.items())
    ]

    start = (query.page - 1) * query.page_size
    end = start + query.page_size
    paged_models = filtered[start:end]

    return JSONResponse(content={
        "code": 200,
        "message": "è¯·æ±‚æˆåŠŸ",
        "total": len(filtered),
        "page": query.page,
        "page_size": query.page_size,
        "models": paged_models
    })

# å®šä¹‰é»˜è®¤æ¨¡å‹æ•°æ®
default_models = [
    # YOLOv5ç³»åˆ—
    {
        "id": 0,
        "name": "yolov5n",
        "task": "detect",
        "size": 3.9,
        "path": "D:/models/yolov5n.pt"
    },
    {
        "id": 1,
        "name": "yolov5s",
        "task": "detect",
        "size": 14.3,
        "path": "D:/models/yolov5s.pt"
    },
    {
        "id": 2,
        "name": "yolov5m",
        "task": "detect",
        "size": 40.8,
        "path": "D:/models/yolov5m.pt"
    },
    {
        "id": 3,
        "name": "yolov5l",
        "task": "detect",
        "size": 90.5,
        "path": "D:/models/yolov5l.pt"
    },
    {
        "id": 4,
        "name": "yolov5n_cls",
        "task": "classify",
        "size": 4.1,
        "path": "D:/models/yolov5n_classify.pt"
    },
    {
        "id": 5,
        "name": "yolov5s_cls",
        "task": "classify",
        "size": 15.2,
        "path": "D:/models/yolov5s_classify.pt"
    },
    {
        "id": 6,
        "name": "yolov5m_cls",
        "task": "classify",
        "size": 42.4,
        "path": "D:/models/yolov5m_classify.pt"
    },
    {
        "id": 7,
        "name": "yolov5n_seg",
        "task": "segment",
        "size": 4.5,
        "path": "D:/models/yolov5n_segment.pt"
    },
    {
        "id": 8,
        "name": "yolov5s_seg",
        "task": "segment",
        "size": 16.8,
        "path": "D:/models/yolov5s_segment.pt"
    },
    {
        "id": 9,
        "name": "yolov5m_seg",
        "task": "segment",
        "size": 43.7,
        "path": "D:/models/yolov5m_segment.pt"
    },
    
    # YOLOv8ç³»åˆ—
    {
        "id": 10,
        "name": "yolov8n",
        "task": "detect",
        "size": 6.2,
        "path": "D:/models/yolov8n.pt"
    },
    {
        "id": 11,
        "name": "yolov8s",
        "task": "detect",
        "size": 15.6,
        "path": "D:/models/yolov8s.pt"
    },
    {
        "id": 12,
        "name": "yolov8m",
        "task": "detect",
        "size": 52.1,
        "path": "D:/models/yolov8m.pt"
    },
    {
        "id": 13,
        "name": "yolov8l",
        "task": "detect",
        "size": 87.4,
        "path": "D:/models/yolov8l.pt"
    },
    {
        "id": 14,
        "name": "yolov8n_cls",
        "task": "classify",
        "size": 5.5,
        "path": "D:/models/yolov8n_classify.pt"
    },
    {
        "id": 15,
        "name": "yolov8s_cls",
        "task": "classify",
        "size": 16.4,
        "path": "D:/models/yolov8s_classify.pt"
    },
    {
        "id": 16,
        "name": "yolov8m_cls",
        "task": "classify",
        "size": 50.7,
        "path": "D:/models/yolov8m_classify.pt"
    },
    {
        "id": 17,
        "name": "yolov8n_seg",
        "task": "segment",
        "size": 7.1,
        "path": "D:/models/yolov8n_segment.pt"
    },
    {
        "id": 18,
        "name": "yolov8s_seg",
        "task": "segment",
        "size": 18.2,
        "path": "D:/models/yolov8s_segment.pt"
    },
    {
        "id": 19,
        "name": "yolov8m_seg",
        "task": "segment",
        "size": 54.9,
        "path": "D:/models/yolov8m_segment.pt"
    },
    
    # YOLOv11ç³»åˆ—
    {
        "id": 20,
        "name": "yolov11n",
        "task": "detect",
        "size": 7.3,
        "path": "D:/models/yolov11n.pt"
    },
    {
        "id": 21,
        "name": "yolov11s",
        "task": "detect",
        "size": 18.7,
        "path": "D:/models/yolov11s.pt"
    },
    {
        "id": 22,
        "name": "yolov11m",
        "task": "detect",
        "size": 55.8,
        "path": "D:/models/yolov11m.pt"
    },
    {
        "id": 23,
        "name": "yolov11l",
        "task": "detect",
        "size": 92.7,
        "path": "D:/models/yolov11l.pt"
    },
    {
        "id": 24,
        "name": "yolov11n_cls",
        "task": "classify",
        "size": 6.8,
        "path": "D:/models/yolov11n_classify.pt"
    },
    {
        "id": 25,
        "name": "yolov11s_cls",
        "task": "classify",
        "size": 19.5,
        "path": "D:/models/yolov11s_classify.pt"
    },
    {
        "id": 26,
        "name": "yolov11m_cls",
        "task": "classify",
        "size": 57.2,
        "path": "D:/models/yolov11m_classify.pt"
    },
    {
        "id": 27,
        "name": "yolov11n_seg",
        "task": "segment",
        "size": 8.2,
        "path": "D:/models/yolov11n_segment.pt"
    },
    {
        "id": 28,
        "name": "yolov11s_seg",
        "task": "segment",
        "size": 20.3,
        "path": "D:/models/yolov11s_segment.pt"
    },
    {
        "id": 29,
        "name": "yolov11m_seg",
        "task": "segment",
        "size": 58.6,
        "path": "D:/models/yolov11m_segment.pt"
    }
]

@app.post("/v1/model/getdefault")
async def get_default_models(query: ModelQuery, authorization: Optional[str] = Header(None)):
    print("ğŸ“¥ æ”¶åˆ°é»˜è®¤æ¨¡å‹åˆ—è¡¨è¯·æ±‚:")
    print(f"- ç­›é€‰æ¡ä»¶: {query.filter}")
    print(f"- é¡µç : {query.page}, æ¯é¡µæ•°é‡: {query.page_size}")
    print(f"- æˆæƒå¤´: {authorization}")

    # æ ¹æ®ç­›é€‰æ¡ä»¶è¿‡æ»¤æ¨¡å‹
    filtered = [
        m for m in default_models
        if all(m.get(k) == v for k, v in query.filter.items())
    ]

    # åˆ†é¡µå¤„ç†
    start = (query.page - 1) * query.page_size
    end = start + query.page_size
    paged_models = filtered[start:end]

    return JSONResponse(content={
        "code": 200,
        "message": "è¯·æ±‚æˆåŠŸ",
        "total": len(filtered),
        "page": query.page,
        "page_size": query.page_size,
        "models": paged_models
    })

# å®šä¹‰æ¨¡å‹è®­ç»ƒè¯·æ±‚ä½“ç»“æ„
class ModelTrainRequest(BaseModel):
    model_id: int
    datasets_id: int
    train_details: Optional[dict] = {}

# å…¨å±€å˜é‡ï¼Œç”¨äºè·Ÿè¸ªè®­ç»ƒçŠ¶æ€
training_status = {}

@app.post("/v1/model/train")
async def model_train(request: ModelTrainRequest, authorization: Optional[str] = Header(None)):
    print("ğŸš€ æ”¶åˆ°æ¨¡å‹è®­ç»ƒè¯·æ±‚:")
    print(f"- æ¨¡å‹ID: {request.model_id}")
    print(f"- æ•°æ®é›†ID: {request.datasets_id}")
    print(f"- è®­ç»ƒç»†èŠ‚: {request.train_details}")
    print(f"- æˆæƒå¤´: {authorization}")

    # åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€
    training_status[request.model_id] = "training"
    
    # è¿™é‡Œåªæ˜¯ç¡®è®¤æ¥æ”¶åˆ°äº†è¯·æ±‚ï¼Œä¸åšå®é™…è®­ç»ƒå¤„ç†
    response_data = {
        "code": 200,
        "message": "æ¨¡å‹è®­ç»ƒè¯·æ±‚å·²æ¥æ”¶",
        "model_id": request.model_id,
        "datasets_id": request.datasets_id,
        "train_details": request.train_details
    }
    
    # å¯åŠ¨å¼‚æ­¥ä»»åŠ¡å‘é€è®­ç»ƒç»“æœ
    # å¯ä»¥é€‰æ‹©ä½¿ç”¨çœŸå®è®­ç»ƒå‡½æ•°æˆ–ç¤ºä¾‹å‡½æ•°
    use_real_training = True
    
    if use_real_training:
        # ä½¿ç”¨çœŸå®è®­ç»ƒå‡½æ•°
        # asyncio.create_task(train_yolo_model(request.model_id, request.datasets_id, request.train_details))
        asyncio.create_task(train_example(request.model_id, request.train_details))
    else:
        # ä½¿ç”¨ç¤ºä¾‹è®­ç»ƒç»“æœ
        asyncio.create_task(send_example_training_results(request.model_id, request.train_details))
    
    return JSONResponse(content=response_data)

# # æš‚åœè®­ç»ƒæ¥å£
# @app.post("/v1/model/train/suspend")
# async def suspend_training(model_id: int, authorization: Optional[str] = Header(None)):
#     print(f"â¸ï¸ æ”¶åˆ°æš‚åœè®­ç»ƒè¯·æ±‚: æ¨¡å‹ID {model_id}")
#     print(f"- æˆæƒå¤´: {authorization}")
    
#     if model_id in training_status:
#         training_status[model_id] = "suspend"
#         return JSONResponse(content={
#             "code": 200,
#             "message": "è®­ç»ƒå·²æš‚åœ",
#             "model_id": model_id
#         })
#     else:
#         return JSONResponse(content={
#             "code": 404,
#             "message": "æœªæ‰¾åˆ°æŒ‡å®šçš„è®­ç»ƒä»»åŠ¡",
#             "model_id": model_id
#         })

# # æ¢å¤è®­ç»ƒæ¥å£
# @app.post("/v1/model/train/resume")
# async def resume_training(model_id: int, authorization: Optional[str] = Header(None)):
#     print(f"â–¶ï¸ æ”¶åˆ°æ¢å¤è®­ç»ƒè¯·æ±‚: æ¨¡å‹ID {model_id}")
#     print(f"- æˆæƒå¤´: {authorization}")
    
#     if model_id in training_status and training_status[model_id] == "suspend":
#         training_status[model_id] = "training"
#         return JSONResponse(content={
#             "code": 200,
#             "message": "è®­ç»ƒå·²æ¢å¤",
#             "model_id": model_id
#         })
#     elif model_id not in training_status:
#         return JSONResponse(content={
#             "code": 404,
#             "message": "æœªæ‰¾åˆ°æŒ‡å®šçš„è®­ç»ƒä»»åŠ¡",
#             "model_id": model_id
#         })
#     else:
#         return JSONResponse(content={
#             "code": 400,
#             "message": "è®­ç»ƒä»»åŠ¡æœªå¤„äºæš‚åœçŠ¶æ€",
#             "model_id": model_id
#         })

# # å–æ¶ˆè®­ç»ƒæ¥å£
# @app.post("/v1/model/train/cancel")
# async def cancel_training(model_id: int, authorization: Optional[str] = Header(None)):
#     print(f"âŒ æ”¶åˆ°å–æ¶ˆè®­ç»ƒè¯·æ±‚: æ¨¡å‹ID {model_id}")
#     print(f"- æˆæƒå¤´: {authorization}")
    
#     if model_id in training_status:
#         # ä»è®­ç»ƒçŠ¶æ€ä¸­ç§»é™¤ï¼Œè¿™ä¼šå¯¼è‡´è®­ç»ƒä»»åŠ¡é€€å‡º
#         del training_status[model_id]
#         return JSONResponse(content={
#             "code": 200,
#             "message": "è®­ç»ƒå·²å–æ¶ˆ",
#             "model_id": model_id
#         })
#     else:
#         return JSONResponse(content={
#             "code": 404,
#             "message": "æœªæ‰¾åˆ°æŒ‡å®šçš„è®­ç»ƒä»»åŠ¡",
#             "model_id": model_id
#         })



# WebSocketè¿æ¥ç®¡ç†å™¨
class ConnectionManager:
    def __init__(self):
        # ä½¿ç”¨å­—å…¸å­˜å‚¨è¿æ¥ï¼Œé”®ä¸ºæ¨¡å‹IDï¼Œå€¼ä¸ºWebSocketè¿æ¥åˆ—è¡¨
        self.active_connections: dict = {}

    async def connect(self, websocket: WebSocket, model_id: int):
        await websocket.accept()
        if model_id not in self.active_connections:
            self.active_connections[model_id] = []
        self.active_connections[model_id].append(websocket)

    def disconnect(self, websocket: WebSocket, model_id: int):
        if model_id in self.active_connections:
            self.active_connections[model_id].remove(websocket)
            # å¦‚æœè¯¥æ¨¡å‹IDæ²¡æœ‰è¿æ¥äº†ï¼Œåˆ™åˆ é™¤è¯¥é”®
            if not self.active_connections[model_id]:
                del self.active_connections[model_id]

    async def broadcast_to_model(self, message: str):
        message_data = json.loads(message)
        model_id = message_data.get("model_id")
        if model_id in self.active_connections:
            print(f"[Broadcast] Model ID: {model_id}, Message: {message}")
            for connection in self.active_connections[model_id]:
                try:
                    client_info = getattr(connection, 'client', 'Unknown client')
                    print(f"  -> Sending to {client_info}")
                    await connection.send_text(message)
                except Exception as e:
                    print(f"  !! Error sending to {client_info}: {e}")

manager = ConnectionManager()

# æ·»åŠ WebSocketç«¯ç‚¹
@app.websocket("/ws/training/{model_id}")
async def websocket_endpoint(websocket: WebSocket, model_id: int):
    await manager.connect(websocket, model_id)
    try:
        # å‘é€è¿æ¥æˆåŠŸæ¶ˆæ¯
        await websocket.send_text(json.dumps({
            "message": "WebSocketè¿æ¥å·²å»ºç«‹",
            "model_id": model_id
        }))
        
        # ä¿æŒè¿æ¥ï¼Œç­‰å¾…æ¶ˆæ¯
        while True:
            try:
                # ç­‰å¾…å®¢æˆ·ç«¯æ¶ˆæ¯ï¼Œä½†ä¸å¤„ç†
                data = await websocket.receive_text()
                # å¯ä»¥åœ¨è¿™é‡Œå¤„ç†å®¢æˆ·ç«¯å‘é€çš„æ¶ˆæ¯ï¼Œå¦‚æœéœ€è¦çš„è¯
            except WebSocketDisconnect:
                break
    except WebSocketDisconnect:
        manager.disconnect(websocket, model_id)
    except Exception as e:
        print(f"WebSocketé”™è¯¯: {str(e)}")
        manager.disconnect(websocket, model_id)

## å·²è¿‡æ—¶,è¯¸å¤šç»†èŠ‚ä¸å½“å‰ç‰ˆæœ¬ä¸ç¬¦
# # å¼‚æ­¥å‡½æ•°ï¼Œç”¨äºå‘é€è®­ç»ƒç»“æœ
# async def send_example_training_results(model_id: int, train_details: dict):
#     # ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œæ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
#     await asyncio.sleep(10)
    
#     # æ£€æŸ¥CSVæ–‡ä»¶æ˜¯å¦å­˜åœ¨
#     csv_path = "car/results.csv"
#     if not os.path.exists(csv_path):
#         print(f"é”™è¯¯: è®­ç»ƒç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
#         return
    
#     # è¯»å–CSVæ–‡ä»¶
#     try:
#         with open(csv_path, 'r', encoding='utf-8') as csvfile:
#             csv_reader = csv.DictReader(csvfile)
#             rows = list(csv_reader)
            
#             # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œå‘é€ç©ºæ•°æ®æ¶ˆæ¯
#             if not rows:
#                 print(f"æ²¡æœ‰è®­ç»ƒç»“æœæ•°æ®: {csv_path}")
#                 return
            
#             # å¤„ç†æ¯ä¸€è¡Œæ•°æ®
#             processed_rows = []
#             for row in rows:
#                 processed_row = {}
#                 for key, value in row.items():
#                     # å»é™¤é”®åä¸­çš„ç©ºæ ¼
#                     clean_key = key.strip()
#                     # å»é™¤å€¼ä¸­çš„ç©ºæ ¼
#                     value = value.strip()
#                     # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
#                     try:
#                         # å°è¯•è½¬æ¢ä¸ºæ•´æ•°
#                         processed_row[clean_key] = int(value)
#                     except ValueError:
#                         try:
#                             # å¦‚æœæ•´æ•°è½¬æ¢å¤±è´¥ï¼Œå°è¯•è½¬æ¢ä¸ºæµ®ç‚¹æ•°
#                             processed_row[clean_key] = float(value)
#                         except ValueError:
#                             # å¦‚æœéƒ½å¤±è´¥ï¼Œä¿æŒåŸå§‹å­—ç¬¦ä¸²
#                             processed_row[clean_key] = value
#                 processed_rows.append(processed_row)
            
#             # è·å–è®­ç»ƒç»†èŠ‚ä¸­çš„epochså­—æ®µ
#             epochs = train_details.get('epochs', 1000)  # é»˜è®¤ä¸º1000
            
#             # æ¯ç§’å‘é€ä¸€è¡Œæ•°æ®
#             for i, row in enumerate(processed_rows):
#                 # æ£€æŸ¥è®­ç»ƒæ˜¯å¦è¢«æš‚åœ
#                 if model_id in training_status and training_status[model_id] == "suspend":
#                     # å‘é€æš‚åœçŠ¶æ€
#                     await manager.broadcast_to_model(model_id, json.dumps({
#                         "message": "è®­ç»ƒå·²æš‚åœ",
#                         "status": "suspend"
#                     }))
#                     # ç­‰å¾…æ¢å¤è®­ç»ƒ
#                     while model_id in training_status and training_status[model_id] == "suspend":
#                         await asyncio.sleep(1)
#                     # å¦‚æœè®­ç»ƒè¢«å–æ¶ˆï¼Œåˆ™é€€å‡º
#                     if model_id not in training_status:
#                         return
                
#                 # ç¡®å®šå½“å‰çŠ¶æ€
#                 if i < len(processed_rows) - 1:
#                     status = "training"  # è¿˜æœ‰æ›´å¤šæ•°æ®ï¼Œä»åœ¨è®­ç»ƒä¸­
#                     # å¹¿æ’­æ¶ˆæ¯ç»™ç‰¹å®šæ¨¡å‹IDçš„å®¢æˆ·ç«¯ï¼ŒåŒ…å«æ•°æ®
#                     message = json.dumps({
#                         "message": "è®­ç»ƒç»“æœæ•°æ®",
#                         "status": status,
#                         "data": row,
#                         "model_id": model_id
#                     })
#                     await manager.broadcast_to_model(model_id, message)
#                 else:
#                     # æœ€åä¸€è¡Œæ•°æ®ï¼Œåˆ¤æ–­æ˜¯å¦æå‰åœæ­¢
#                     current_epoch = int(row.get('epoch', 0))
#                     if current_epoch < epochs:
#                         status = "early_stop"  # æå‰åœæ­¢
#                     else:
#                         status = "finished"  # æ­£å¸¸å®Œæˆ
                    
#                     # å‘é€æœ€åä¸€æ¡æ•°æ®ï¼ŒåŒ…å«çŠ¶æ€å’Œæ•°æ®
#                     message = json.dumps({
#                         "message": "è®­ç»ƒç»“æœæ•°æ®",
#                         "status": training,
#                         "data": row,
#                         "model_id": model_id
#                     })
#                     await manager.broadcast_to_model(model_id, message)
                    
#                     # å‘é€å®ŒæˆçŠ¶æ€ï¼Œä¸åŒ…å«æ•°æ®
#                     message = json.dumps({
#                         "message": "è®­ç»ƒç»“æœæ•°æ®å‘é€å®Œæˆ",
#                         "status": status,
#                         "model_id": model_id
#                     })
#                     await manager.broadcast_to_model(model_id, message)
                
#                 await asyncio.sleep(1)  # æ¯ç§’å‘é€ä¸€æ¬¡
            
#             # æ¸…ç†è®­ç»ƒçŠ¶æ€
#             if model_id in training_status:
#                 del training_status[model_id]
            
#     except Exception as e:
#         print(f"å‘é€è®­ç»ƒç»“æœæ—¶å‡ºé”™: {str(e)}")
#         # æ¸…ç†è®­ç»ƒçŠ¶æ€
#         if model_id in training_status:
#             del training_status[model_id]


def websocket_publish(message):
    if main_event_loop:
        asyncio.run_coroutine_threadsafe(message_queue.put(message), main_event_loop)
    else:
        print("ä¸»äº‹ä»¶å¾ªç¯æœªè®¾ç½®ï¼Œæ— æ³•å‘é€æ¶ˆæ¯ã€‚")

def on_train_epoch_end(model_id:int):
    def callback(trainer):
        print("è¿›å…¥è®­ç»ƒåå›è°ƒå‡½æ•°")
    
        # curr_epoch = trainer.epoch + 1
        # text = f"Epoch Number: {curr_epoch}/{trainer.epochs} finished"
        # print(text)
        # print("-" * 50)

        # instance_variables = vars(trainer)
        # print(instance_variables)
        
        try:
            loss_dict = trainer.label_loss_items(trainer.tloss, prefix="train")
            metrics = trainer.metrics if trainer.metrics else {}
            # print("trainer.metrics keys:", trainer.metrics.keys())
            
            data = {
                "epoch": trainer.epoch + 1,
                "train/box_loss": loss_dict.get('train/box_loss'),
                "train/dfl_loss": loss_dict.get('train/dfl_loss'),
                "train/cls_loss": loss_dict.get('train/cls_loss'),
                "metrics/precision": metrics.get('metrics/precision(B)'),
                "metrics/recall": metrics.get('metrics/recall(B)'),
                "metrics/mAP_0.5": metrics.get('metrics/mAP50(B)'),
                "metrics/mAP_0.5:0.95": metrics.get('metrics/mAP50-95(B)'),
                "val/box_loss": metrics.get('val/box_loss'),
                "val/dfl_loss": metrics.get('val/dfl_loss'),
                "val/cls_loss": metrics.get('val/cls_loss'),
            }
        
            # æ„å»ºæ¶ˆæ¯
            message = json.dumps({
                "message": "è®­ç»ƒç»“æœæ•°æ®",
                "status": "training",
                "data": data,
                "model_id": model_id
            })
            websocket_publish(message)
            
        except Exception as e:
            print(f"å‘é€è®­ç»ƒå›è°ƒæ•°æ®æ—¶å‡ºé”™: {str(e)}")
            
            
    return callback
    
        
# def on_train_end(trainer):
#     # åˆ‡æ¢è®­ç»ƒçŠ¶æ€
#     model_id = 0

#     current_epoch = trainer.epoch + 1
#     max_epochs = trainer.epochs
    
#     # å¦‚æœæ˜¯æœ€åä¸€ä¸ªepochï¼ŒçŠ¶æ€ä¸ºfinished
#     if current_epoch >= max_epochs:
#         status = "finished"
#         training_status[model_id] = "finished"
#         print(f"\nè®­ç»ƒå®Œæˆï¼æ€»è½®æ¬¡: {current_epoch}/{max_epochs}")
#     # æ£€æŸ¥æ˜¯å¦æ—©åœï¼ˆå¦‚æœæœ‰early_stopæ ‡å¿—æˆ–è€…é€šè¿‡å…¶ä»–æ¡ä»¶åˆ¤æ–­ï¼‰
#     elif current_epoch < max_epochs:
#         status = "early_stop"
#         training_status[model_id] = "early_stop"
#         print(f"\nè®­ç»ƒæå‰ç»ˆæ­¢ï¼è½®æ¬¡: {current_epoch}/{max_epochs}")
    
#     # è®¡ç®—æ•´ä½“è®­ç»ƒæ—¶é—´
#     total_training_time = trainer.end_time - trainer.start_time
#     print(f"è®­ç»ƒç»“æŸï¼æ¨¡å‹ID: {model_id}")
#     print(f"æ€»è®­ç»ƒæ—¶é—´: {total_training_time}")
    
#     message = json.dumps({
#         "message": "è®­ç»ƒå®Œæˆ",
#         "status": status,
#         "total_training_time": str(total_training_time),
#         "best_epoch": best_epoch,
#         "best_accuracy": best_accuracy,
#         "best_model_path": trainer.best
#     })
    
#     websocket_publish(message)

#     print(f"æœ€ä½³è®­ç»ƒæ‰¹æ¬¡æ¨¡å‹åœ°å€:{trainer.best} ç²¾åº¦: {trainer.best_accuracy}")
    
#     if model_id in training_status:
#         del training_status[model_id]
    
        
async def train_example(model_id: int, train_details: dict):
    await asyncio.sleep(1)  # ç­‰å¾…å‰ç«¯å»ºç«‹websocketè¿æ¥
    
    # å¯åŠ¨çº¿ç¨‹å¹¶ä¼ é€’å‚æ•°
    threading.Thread(target=run_training_process, args=(model_id, train_details), daemon=True).start()

def run_training_process(model_id: int, train_details: dict):
    # åˆå§‹åŒ–æ¨¡å‹
    model = YOLO("weights/yolov8/yolov8s.pt", task="detect")
    
    # æ·»åŠ å›è°ƒå‡½æ•°
    model.add_callback("on_train_epoch_end", on_train_epoch_end(model_id))
    # model.add_callback("on_train_end", on_train_end)
    
    # æ›´æ–°è®­ç»ƒçŠ¶æ€
    training_status[model_id] = "training"
    
    # ç›´æ¥ä½¿ç”¨train_detailsä¸­çš„å‚æ•°è¿›è¡Œè®­ç»ƒ
    model.train(
        data=r"/home/cvrsg/rs_workspace/aihub/backend_simp/datasets/split_car_dataset/sar.yaml",
        **train_details
    )
    
@app.on_event("startup")
async def start_background_task():
    global main_event_loop
    main_event_loop = asyncio.get_event_loop()

    async def broadcaster():
        while True:
            msg = await message_queue.get()
            await manager.broadcast_to_model(msg)

    asyncio.create_task(broadcaster())